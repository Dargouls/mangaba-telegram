#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cliente Gemini AI - M√©dico de Bolso
Integra√ß√£o com Google Gemini para processamento de consultas m√©dicas
Sistema de Fallback com m√∫ltiplas chaves de API e modelos
"""

import logging
import asyncio
import time
import google.generativeai as genai
from typing import List, Dict, Any, Optional, Tuple
from src.config.settings import GEMINI_API_KEYS, GEMINI_MODELS
from src.ai.conversation_agents import ConversationManager

logger = logging.getLogger(__name__)

class GeminiMedicalAI:
    """Cliente para integra√ß√£o com Gemini AI com sistema de fallback"""
    
    def __init__(self):
        """Inicializa o cliente Gemini com sistema de fallback"""
        self.api_keys = GEMINI_API_KEYS.copy()
        self.models = GEMINI_MODELS.copy()
        self.current_api_index = 0
        self.current_model_index = 0
        self.failed_combinations = set()  # Track failed API+Model combinations
        self.rate_limit_cooldowns = {}  # Track rate limit cooldowns
        self.medical_prompt = self._create_medical_prompt()
        
        # Initialize conversation manager for dynamic responses
        self.conversation_manager = ConversationManager()
        
        # Initialize first working combination
        self.current_client = None
        self._initialize_client()
        
        logger.info(f"Cliente Gemini AI inicializado com {len(self.api_keys)} chaves e {len(self.models)} modelos")
    
    def _initialize_client(self) -> bool:
        """Inicializa cliente com a primeira combina√ß√£o dispon√≠vel"""
        for api_idx, api_key in enumerate(self.api_keys):
            for model_idx, model_name in enumerate(self.models):
                combination = (api_idx, model_idx)
                
                # Skip failed combinations
                if combination in self.failed_combinations:
                    continue
                    
                # Check rate limit cooldown
                if self._is_rate_limited(combination):
                    continue
                
                try:
                    genai.configure(api_key=api_key)
                    self.current_client = genai.GenerativeModel(model_name)
                    self.current_api_index = api_idx
                    self.current_model_index = model_idx
                    
                    logger.info(f"Inicializado com API {api_idx+1} e modelo {model_name}")
                    return True
                    
                except Exception as e:
                    logger.warning(f"Falha ao inicializar API {api_idx+1} com modelo {model_name}: {e}")
                    self.failed_combinations.add(combination)
                    continue
        
        logger.error("Nenhuma combina√ß√£o de API/modelo dispon√≠vel")
        return False
    
    def _create_medical_prompt(self) -> str:
        """Cria o prompt base para consultas m√©dicas com suporte a conversa√ß√£o din√¢mica"""
        return """
Voc√™ √© o "M√©dico de Bolso" ü©∫, um assistente m√©dico virtual inteligente especializado em triagem inicial.
Voc√™ tem personalidade acolhedora, √© direto quando necess√°rio e adapta seu estilo de comunica√ß√£o ao contexto.

PERSONALIDADE E COMUNICA√á√ÉO:
- Seja EMP√ÅTICO e ACOLHEDOR sempre
- Adapte o tom: mais direto para emerg√™ncias, mais detalhado para casos complexos
- Use linguagem simples e acess√≠vel
- Seja CONCISO quando solicitado (modo quick)
- Mantenha conversas FLUIDAS e NATURAIS

DIRETRIZES M√âDICAS:
1. TRIAGEM INICIAL apenas - nunca diagn√≥sticos definitivos
2. Identifique URG√äNCIA e aja adequadamente
3. Recomende atendimento m√©dico quando apropriado
4. Use perguntas inteligentes para entender sintomas
5. Seja CLARO sobre limita√ß√µes

SINAIS DE EMERG√äNCIA (atendimento IMEDIATO):
üö® Dor no peito intensa
üö® Dificuldade respirat√≥ria severa
üö® Perda de consci√™ncia
üö® Sangramento intenso
üö® Febre >39¬∞C persistente
üö® Sintomas neurol√≥gicos

ESTILO DE RESPOSTA:
- Use emojis apropriados (ü©∫üíä‚ö†Ô∏èüè•)
- Seja DIRETO em emerg√™ncias
- Fa√ßa perguntas de follow-up inteligentes
- Mantenha conversas CURTAS e FLUIDAS
- Termine sempre com orienta√ß√£o m√©dica

Lembre-se: Voc√™ √© um assistente inteligente de triagem que se adapta ao usu√°rio e √† situa√ß√£o.
"""
    
    def _is_rate_limited(self, combination: Tuple[int, int]) -> bool:
        """Verifica se uma combina√ß√£o est√° em cooldown por rate limit"""
        if combination not in self.rate_limit_cooldowns:
            return False
        
        cooldown_until = self.rate_limit_cooldowns[combination]
        if time.time() < cooldown_until:
            return True
        
        # Remove expired cooldown
        del self.rate_limit_cooldowns[combination]
        return False
    
    def _set_rate_limit_cooldown(self, combination: Tuple[int, int], cooldown_seconds: int = 60):
        """Define cooldown para uma combina√ß√£o que atingiu rate limit"""
        self.rate_limit_cooldowns[combination] = time.time() + cooldown_seconds
        logger.warning(f"Rate limit atingido para API {combination[0]+1} modelo {self.models[combination[1]]}, cooldown de {cooldown_seconds}s")
    
    def _switch_to_next_combination(self) -> bool:
        """Muda para pr√≥xima combina√ß√£o dispon√≠vel"""
        current_combination = (self.current_api_index, self.current_model_index)
        
        # Mark current combination as failed
        self.failed_combinations.add(current_combination)
        
        # Try to find next working combination
        for api_idx, api_key in enumerate(self.api_keys):
            for model_idx, model_name in enumerate(self.models):
                combination = (api_idx, model_idx)
                
                # Skip current and failed combinations
                if combination == current_combination or combination in self.failed_combinations:
                    continue
                    
                # Check rate limit cooldown
                if self._is_rate_limited(combination):
                    continue
                
                try:
                    genai.configure(api_key=api_key)
                    self.current_client = genai.GenerativeModel(model_name)
                    self.current_api_index = api_idx
                    self.current_model_index = model_idx
                    
                    logger.info(f"Mudan√ßa para API {api_idx+1} e modelo {model_name}")
                    return True
                    
                except Exception as e:
                    logger.warning(f"Falha ao mudar para API {api_idx+1} com modelo {model_name}: {e}")
                    self.failed_combinations.add(combination)
                    continue
        
        logger.error("Nenhuma combina√ß√£o alternativa dispon√≠vel")
        return False
    
    async def process_medical_query(
        self, 
        user_message: str, 
        user_id: str = None,
        session_history: List[Dict[str, Any]] = None,
        triage_data: Dict[str, Any] = None
    ) -> str:
        """Processa consulta m√©dica usando Gemini AI com sistema de fallback e agentes de conversa√ß√£o"""
        
        # Usar agentes de conversa√ß√£o para respostas din√¢micas
        if user_id:
            quick_response, needs_full_ai = await self.conversation_manager.process_message(
                user_id, user_message, triage_data
            )
            
            # Se n√£o precisa da IA completa, retornar resposta r√°pida
            if not needs_full_ai:
                logger.info(f"Resposta r√°pida gerada para usu√°rio {user_id}")
                return quick_response
        
        # Processar com IA completa para casos complexos
        max_retries = len(self.api_keys) * len(self.models)
        
        for attempt in range(max_retries):
            try:
                if not self.current_client:
                    if not self._initialize_client():
                        break
                
                # Construir contexto da conversa
                conversation_context = self._build_conversation_context(
                    user_message, session_history, triage_data, user_id
                )
                
                # Gerar resposta
                response = await self._generate_response_with_retry(conversation_context)
                
                if response:
                    # Adaptar resposta baseada no contexto do usu√°rio
                    if user_id:
                        context = self.conversation_manager.context_agent.get_or_create_context(user_id)
                        response = self.conversation_manager.response_agent.adapt_response_style(response, context)
                    
                    return response
                    
            except Exception as e:
                logger.error(f"Erro na tentativa {attempt + 1}: {e}")
                
                # Try to switch to next combination
                if not self._switch_to_next_combination():
                    break
        
        logger.error("Todas as combina√ß√µes de API/modelo falharam")
        return self._get_fallback_response()
    
    def _build_conversation_context(self, user_message: str, session_history: List[Dict], triage_data: Dict, user_id: str = None) -> str:
        """Constr√≥i o contexto da conversa para o Gemini com informa√ß√µes din√¢micas"""
        context_parts = [self.medical_prompt]
        
        # Adicionar dados de triagem se dispon√≠veis
        if triage_data:
            context_parts.append(f"\nDADOS DE TRIAGEM INICIAL:\n{self._format_triage_data(triage_data)}")
        
        # Adicionar hist√≥rico da sess√£o
        if session_history:
            context_parts.append("\nHIST√ìRICO DA CONVERSA:")
            for msg in session_history[-5:]:  # √öltimas 5 mensagens
                role = "Paciente" if msg['role'] == 'user' else "M√©dico de Bolso"
                context_parts.append(f"{role}: {msg['content']}")
        
        # Adicionar mensagem atual
        context_parts.append(f"\nMENSAGEM ATUAL DO PACIENTE:\n{user_message}")
        
        # Instru√ß√£o final
        context_parts.append(
            "\nResponda como o M√©dico de Bolso, fornecendo orienta√ß√£o m√©dica inicial apropriada."
        )
        
        # Adicionar informa√ß√µes de contexto din√¢mico se dispon√≠vel
        if user_id:
            context = self.conversation_manager.context_agent.get_or_create_context(user_id)
            stats = self.conversation_manager.get_conversation_stats(user_id)
            
            context_parts.append(f"\nCONTEXTO DA CONVERSA:")
            context_parts.append(f"N√∫mero de mensagens: {stats['message_count']}")
            context_parts.append(f"N√≠vel de urg√™ncia atual: {stats['urgency_level']}")
            context_parts.append(f"Sintomas detectados: {', '.join(stats['symptoms']) if stats['symptoms'] else 'Nenhum'}")
            context_parts.append(f"Modo de conversa: {stats['conversation_mode']}")
            
            # Ajustar prompt baseado no modo de conversa
            if stats['conversation_mode'] == 'quick':
                context_parts.append("\nINSTRU√á√ÉO ESPECIAL: Forne√ßa uma resposta CONCISA e DIRETA. M√°ximo 2-3 frases.")
            elif stats['conversation_mode'] == 'emergency':
                context_parts.append("\nINSTRU√á√ÉO ESPECIAL: SITUA√á√ÉO DE EMERG√äNCIA! Seja direto e enf√°tico sobre a necessidade de atendimento imediato.")
        
        return "\n".join(context_parts)
    
    def _format_triage_data(self, triage_data: Dict) -> str:
        """Formata dados de triagem para o contexto"""
        if not triage_data:
            return "Nenhum dado de triagem dispon√≠vel"
        
        formatted = []
        if 'urgency_level' in triage_data:
            formatted.append(f"N√≠vel de urg√™ncia: {triage_data['urgency_level']}")
        if 'symptoms_detected' in triage_data:
            formatted.append(f"Sintomas detectados: {', '.join(triage_data['symptoms_detected'])}")
        if 'risk_factors' in triage_data:
            formatted.append(f"Fatores de risco: {', '.join(triage_data['risk_factors'])}")
        
        return "\n".join(formatted)
    
    async def _generate_response_with_retry(self, context: str) -> str:
        """Gera resposta com tratamento de rate limits e fallback"""
        try:
            response = await self.current_client.generate_content_async(context)
            
            if response and response.text:
                return self._format_response(response.text)
            else:
                logger.warning("Resposta vazia do modelo")
                return None
                
        except Exception as e:
            error_message = str(e).lower()
            current_combination = (self.current_api_index, self.current_model_index)
            
            # Check for rate limit errors
            if any(keyword in error_message for keyword in ['rate limit', 'quota', 'too many requests', '429']):
                logger.warning(f"Rate limit detectado: {e}")
                self._set_rate_limit_cooldown(current_combination, 300)  # 5 minutes cooldown
                return None
            
            # Check for quota exceeded
            elif any(keyword in error_message for keyword in ['quota exceeded', 'billing', 'credits']):
                logger.warning(f"Quota/cr√©ditos esgotados: {e}")
                self.failed_combinations.add(current_combination)
                return None
            
            # Check for authentication errors
            elif any(keyword in error_message for keyword in ['authentication', 'api key', 'unauthorized', '401']):
                logger.error(f"Erro de autentica√ß√£o: {e}")
                self.failed_combinations.add(current_combination)
                return None
            
            # Other errors
            else:
                logger.error(f"Erro inesperado: {e}")
                return None
    
    async def _generate_response(self, context: str) -> str:
        """Gera resposta usando Gemini AI"""
        try:
            response = self.model.generate_content(context)
            
            if response.text:
                return self._format_response(response.text)
            else:
                logger.warning("Resposta vazia do Gemini AI")
                return self._get_fallback_response()
                
        except Exception as e:
            logger.error(f"Erro ao gerar resposta com Gemini: {e}")
            return self._get_fallback_response()
    
    def _format_response(self, response: str) -> str:
        """Formata a resposta do Gemini"""
        # Limitar tamanho da resposta
        if len(response) > 2000:
            response = response[:1950] + "\n\n[...resposta truncada...]"
        
        # Adicionar disclaimer se n√£o estiver presente
        if "consulta m√©dica" not in response.lower():
            response += "\n\n‚ö†Ô∏è *Lembre-se:* Esta √© apenas uma orienta√ß√£o inicial. Consulte um m√©dico para avalia√ß√£o completa."
        
        return response
    
    def reset_failed_combinations(self):
        """Reseta combina√ß√µes falhadas (√∫til para testes ou ap√≥s resolver problemas)"""
        self.failed_combinations.clear()
        self.rate_limit_cooldowns.clear()
        logger.info("Combina√ß√µes falhadas resetadas")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Retorna status atual do sistema de fallback"""
        current_combination = (self.current_api_index, self.current_model_index)
        
        status = {
            "current_api": self.current_api_index + 1,
            "current_model": self.models[self.current_model_index],
            "total_apis": len(self.api_keys),
            "total_models": len(self.models),
            "failed_combinations": len(self.failed_combinations),
            "rate_limited_combinations": len([c for c in self.rate_limit_cooldowns.keys() if self._is_rate_limited(c)]),
            "available_combinations": 0
        }
        
        # Count available combinations
        for api_idx in range(len(self.api_keys)):
            for model_idx in range(len(self.models)):
                combination = (api_idx, model_idx)
                if combination not in self.failed_combinations and not self._is_rate_limited(combination):
                    status["available_combinations"] += 1
        
        return status
    
    def _get_fallback_response(self) -> str:
        """Retorna resposta de fallback quando AI n√£o est√° dispon√≠vel"""
        status = self.get_system_status()
        
        if status["available_combinations"] > 0:
            message = (
                "ü§ñ *Assistente M√©dico Temporariamente Indispon√≠vel*\n\n"
                "Estou enfrentando dificuldades t√©cnicas no momento. "
                "Por favor, tente novamente em alguns minutos.\n\n"
            )
        else:
            message = (
                "ü§ñ *Assistente M√©dico Indispon√≠vel*\n\n"
                "Todos os servi√ßos de IA est√£o temporariamente indispon√≠veis. "
                "Isso pode ser devido a limites de uso ou problemas t√©cnicos.\n\n"
                "Por favor, tente novamente mais tarde.\n\n"
            )
        
        message += (
            "‚ö†Ô∏è **Em caso de emerg√™ncia m√©dica, procure atendimento presencial imediatamente "
            "ou ligue para o SAMU (192).**"
        )
        
        return message